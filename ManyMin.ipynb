{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The spring vibration with mass m and stiffness k has a wave like known solution. The following equation was solved with k/m set to one.\n",
    "\\begin{eqnarray} x'' +(k/m)*x &=& 0 \\\\ x(0) &=& 1 \\\\ x'(0) &=& 0  \\end{eqnarray}\n",
    "It is clearly seen from the plot that displacement function, in blue, goes thru three maxima and three minima. If we try to solve \n",
    "this problem with neural network schemes which minimizes a loss function, reference https://https://kitchingroup.cheme.cmu.edu/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd/,\n",
    "we are going to run into a problem. The doctrine of minimization is just one minimum during a training. This is obvious as for more than one minimum, the optimizer which keeps on trying to minimize several spots, with gradient descent method,  stops converging. \n",
    "Let us examine this case with one of the optimizer methods. This is very well written, very robust, non shooting, gradient descent minimization method. It has been applied to many fluid and thermal heat transfer problems with great success. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from time import process_time\n",
    "!pip install autograd\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.misc.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def f1(x1,x2,x3,t):\n",
    "  dx1dt = x2\n",
    "  return dx1dt\n",
    "\n",
    "def f2(x1,x2,x3,t):\n",
    "##  dx2dt = (t**2 - 3.0)*x1\n",
    "  dx2dt = -x1\n",
    "  return dx2dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from time import process_time\n",
    "import matplotlib.pyplot as plt\n",
    "betaList = [0., 0.]\n",
    "def solve(t0,tn,n):\n",
    "  mt = 0\n",
    "  mnt = 1\n",
    "  datSet = 0\n",
    "  beta = betaList.pop(0)\n",
    "    \n",
    "##Plotting parameters\n",
    "  r = []\n",
    "  y = []\n",
    "  z = []\n",
    "  zz = []\n",
    "\n",
    "  \n",
    "  while betaList != []:\n",
    "    mt = 0\n",
    "    datSet += 1\n",
    "    \n",
    "    \n",
    "    while (mt<mnt):   \n",
    "      x1= 1.0\n",
    "      x2 = 0.0\n",
    "      \n",
    "      mt += 1\n",
    "      dt=(tn-t0)/n\n",
    "      t = t0\n",
    "      \n",
    "      x3 , k31, k32, k33 = 0., 0., 0., 0.\n",
    "      for i in range (n):\n",
    "\n",
    "         k11 = dt*f1(x1,x2,x3,t)\n",
    "         k21 = dt*f2(x1,x2,x3,t)\n",
    "         k12 = dt*f1(x1+0.5*k11,x2+0.5*k21,x3+0.5*k31,t+0.5*dt)\n",
    "         k22 = dt*f2(x1+0.5*k11,x2+0.5*k21,x3+0.5*k31,t+0.5*dt)\n",
    "         k13 = dt*f1(x1+0.5*k12,x2+0.5*k22,x3+0.5*k32,t+0.5*dt)\n",
    "         k23 = dt*f2(x1+0.5*k12,x2+0.5*k22,x3+0.5*k32,t+0.5*dt)\n",
    "         k14 = dt*f1(x1+k13,x2+k23,x3+k33,t+dt)\n",
    "         k24 = dt*f2(x1+k13,x2+k23,x3+k33,t+dt)\n",
    "         x1 += (k11+2*k12+2*k13+k14)/6\n",
    "         x2 += (k21+2*k22+2*k23+k24)/6\n",
    "         t += dt\n",
    "\n",
    "         # plotting data\n",
    "         r = [t] + r\n",
    "         y = [x1] + y\n",
    "         z = [x2]+ z\n",
    "                  \n",
    "    ##   One set done, start next beta from the list\n",
    "    beta = betaList.pop(0)\n",
    "\n",
    "\n",
    "  fig, ay = plt.subplots()\n",
    "  ay.plot(r,y, 'b',  label = \"displacement f\")\n",
    "  ay.plot(r, z, 'r', label = \"f'\")\n",
    "\n",
    "  legend = ay.legend(loc='best', shadow=True, fontsize='x-small', title='spring load')\n",
    "  legend.get_frame().set_facecolor('white')\n",
    "  plt.show()\n",
    "##\n",
    "\n",
    "## All the plots are stored in images directory of run directory  \n",
    "if __name__ == '__main__':\n",
    "  beta = 0.0\n",
    "  t1_start = process_time()\n",
    "\n",
    "  solve(0, 16, 400)\n",
    "  \n",
    "  t1_stop = process_time()\n",
    "  print('Elapsed run time in secs: ' , t1_stop - t1_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy.random as npr\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.misc.optimizers import adam\n",
    "def init_random_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples, one for each layer.\"\"\"\n",
    "    \n",
    "    return [(rs.randn(insize, outsize) * scale,   # weight matrix\n",
    "             rs.randn(outsize) * scale)           # bias vector\n",
    "            for insize, outsize in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    \"see https://arxiv.org/pdf/1710.05941.pdf\"\n",
    "    return x / (1.0 + np.exp(-x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from autograd import grad, elementwise_grad\n",
    "\n",
    "layer_sizes=[1, 8, 1]\n",
    "params = init_random_params(0.1, layer_sizes)\n",
    "\n",
    "def f(params, inputs):\n",
    "    \"Neural network functions\"\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = swish(outputs)    \n",
    "    return outputs\n",
    "\n",
    "# Derivatives\n",
    "fp = elementwise_grad(f, 1)\n",
    "fpp = elementwise_grad(fp, 1)\n",
    "fppp = elementwise_grad(fpp, 1)\n",
    "eta = np.linspace(0, 4.*np.pi).reshape((-1, 1))\n",
    "\n",
    "## This is the function we seek to minimize\n",
    "def objective(params, step):\n",
    "    # These should all be zero at the solution\n",
    "    zeq = fpp(params, eta) +  f(params, eta) \n",
    "    bc0 = f(params, 0.0) - 1.0 # equal to zero at solution\n",
    "    bc1 = fp(params, 0.0)  # equal to zero at solution\n",
    "    return np.mean(zeq**2) + bc0**2 + bc1**2\n",
    "\n",
    "from autograd.misc.optimizers import adam\n",
    "def callback(params, step, g):\n",
    "    if step % 1000 == 0:\n",
    "        print(\"Iteration {0:3d} objective {1}\".format(step,\n",
    "                                                      objective(params, step)))\n",
    "\n",
    "params = adam(grad(objective), params,\n",
    "              step_size=0.001, num_iters=10000, callback=callback)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('f(0) = {}'.format(f(params, 0.0)))\n",
    "print('fp(0) = {}'.format(fp(params, 0.0)))\n",
    "print('fp(inf) = {}'.format(f(params, 4.*np.pi)))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(eta, f(params, eta), 'b', label = \"f\" )\n",
    "plt.plot(eta, fp(params, eta), 'g', label = \"f'\" )\n",
    "plt.xlim([0, 14])\n",
    "plt.ylim([-1., 1.])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is obviously we did not expect. The process did not converge and the method could not find even one minimum. Since I am not a machine language scholar, as a user I would like to get some engineering fix for the problem(you may call it band aid fix). I tried various things, the one fix which worked was to split the domain in half, and solve the same equation in each domain with the connected boundary condition. The following code illustrates the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install autograd\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.misc.optimizers import adam\n",
    "\n",
    "def init_random_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples, one for each layer.\"\"\"\n",
    "    \n",
    "    return [(rs.randn(insize, outsize) * scale,   # weight matrix\n",
    "             rs.randn(outsize) * scale)           # bias vector\n",
    "            for insize, outsize in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    \"see https://arxiv.org/pdf/1710.05941.pdf\"\n",
    "    return x / (1.0 + np.exp(-x))\n",
    "\n",
    "layer_sizes=[1, 8, 1]\n",
    "params = init_random_params(0.1, layer_sizes)\n",
    "\n",
    "def f(params, inputs):\n",
    "    \"Neural network functions\"\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = swish(outputs)    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "# Derivatives\n",
    "fp = elementwise_grad(f, 1)\n",
    "fpp = elementwise_grad(fp, 1)\n",
    "fppp = elementwise_grad(fpp, 1)\n",
    "\n",
    "## This is the function we seek to minimize\n",
    "def objective(params, step):\n",
    "    # These should all be zero at the solution\n",
    "    zeq = fpp(params, eta) +  f(params, eta) \n",
    "    bc0 = f(params, 0.0) - 1.0 # equal to zero at solution\n",
    "    bc1 = fp(params, 0.0)  # equal to zero at solution\n",
    "    return np.mean(zeq**2) + bc0**2 + bc1**2\n",
    "\n",
    "\n",
    "def ft(params2, inputs):\n",
    "    \"Neural network functions\"\n",
    "    for W2, b2 in params2:\n",
    "        outputs = np.dot(inputs-2.*np.pi, W2) + b2\n",
    "        inputs = swish(outputs)    \n",
    "    return outputs\n",
    "\n",
    "layer_sizes=[1, 8, 1]\n",
    "##params = init_random_params(0.1, layer_sizes)\n",
    "params2 = init_random_params(0.1, layer_sizes)\n",
    "\n",
    "ftp = elementwise_grad(ft, 1)\n",
    "ftpp = elementwise_grad(ftp, 1)\n",
    "ftppp = elementwise_grad(ftpp, 1)\n",
    "\n",
    "eta = np.linspace(0, 2.*np.pi).reshape((-1, 1))\n",
    "eta2 = np.linspace(2.*np.pi, 4.*np.pi).reshape((-1, 1))\n",
    "\n",
    "def objective2(params2, step):\n",
    "    zeq2 = ftpp(params2, eta2) +  ft(params2, eta2) \n",
    "    bc0 = ft(params2, 2.*np.pi) - f(params,2.*np.pi) # equal to zero at solution\n",
    "    bc1 = ftp(params2, 2.*np.pi) - fp(params, 2.*np.pi) # equal to zero at solution\n",
    "    return np.mean(zeq2**2) + bc0**2 + bc1**2\n",
    "\n",
    "from autograd.misc.optimizers import adam\n",
    "def callback(params, step, g):\n",
    "    if step % 1000 == 0:\n",
    "        print(\"Iteration {0:3d} objective {1}\".format(step,\n",
    "                                                      objective(params, step)))\n",
    "def callback2(params2, step, g):\n",
    "    if step % 1000 == 0:\n",
    "        print(\"Iteration {0:3d} objective {1}\".format(step,\n",
    "                                                      objective2(params2, step)))\n",
    "for k in range(5):\n",
    "  params = adam(grad(objective), params,\n",
    "              step_size=0.001, num_iters=4000, callback=callback)\n",
    "              \n",
    "  params2 = adam(grad(objective2), params2,\n",
    "              step_size=0.001, num_iters=4000, callback=callback2)\n",
    "print('fp(inf) = {}'.format(ft(params2, 4.*np.pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(eta, f(params, eta), 'b', label = \"f\" )\n",
    "plt.plot(eta2, ft(params2, eta2), 'r', label = \"f\")\n",
    "plt.plot(eta, fp(params, eta), 'g', label = \"f'\" )\n",
    "plt.plot(eta2, ftp(params2, eta2), 'm', label = \"f'\" )\n",
    "plt.xlim([0, 4.*np.pi])\n",
    "plt.ylim([-1., 1.])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Great. Hopefully we resolved one way to solve such problems. Now we will look into another way to solve Shrodinger equation, reference https://kitchingroup.cheme.cmu.edu/blog/2017/11/29/Solving-an-eigenvalue-differential-equation-with-a-neural-network/.\n",
    "Author Prof. Kitchin gives an elaborate presentation of the method for several values of eigenvalue associated with the wave.  As a refresher I quote the scope of the problem mentioned in that reference.\n",
    "\" The 1D harmonic oscillator is described here. It is a boundary value differential equation with eigenvalues. If we let let ω=1, m=1, and units where ℏ=1. then, the governing differential equation becomes:\n",
    "\n",
    "$(-0.5 \\frac{d^2\\psi(x)}{dx^2} + (0.5 x^2 - E) \\psi(x) = 0)$\n",
    "\n",
    "with boundary conditions:$ (\\psi(-\\infty) = \\psi(\\infty) = 0)$\n",
    "\n",
    "We can further stipulate that the probability of finding the particle over this domain is equal to one: $(\\int_{-\\infty}^{\\infty} \\psi^2(x) dx = 1)$. In this set of equations, $(E)$ is an eigenvalue, which means there are only non-trivial solutions for certain values of $(E)$.\n",
    "\n",
    "Our goal is to solve this equation using a neural network to represent the wave function. This is a different problem than the one here or here because of the eigenvalue. This is an additional adjustable parameter we have to find. Also, we have the normalization constraint to consider, which we did not consider before.\"\n",
    "\n",
    "I am going to concentrate on the first excited state with eigenvalue E of 1.5. The use of some initial wave shape to train or pretrain the gradient function takes out the beauty of the method. I am going to propose either using  additional boundary conditions at the min and max value of the function or just using the pobability function. This works beautifully with no prior knowledge of the solution. Here is my code to do this. \n",
    "Remember to restart and clear the kernel when solving a new problem. Thanks for patiently reading this long blog. I will appreciate constructive comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.misc.optimizers import adam\n",
    "\n",
    "def init_random_params(scale, layer_sizes, rs=npr.RandomState(42)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples, one for each layer.\"\"\"\n",
    "    \n",
    "    return [(rs.randn(insize, outsize) * scale,   # weight matrix\n",
    "             rs.randn(outsize) * scale)           # bias vector\n",
    "            for insize, outsize in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    \"see https://arxiv.org/pdf/1710.05941.pdf\"\n",
    "    return x / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def f(params, inputs):\n",
    "    \"Neural network functions\"\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = swish(outputs)    \n",
    "    return outputs\n",
    "\n",
    "    \n",
    "# Here is our initial guess of params:\n",
    "layer_sizes=[1, 8, 1]\n",
    "params = init_random_params(0.1, layer_sizes)\n",
    "\n",
    "# Derivatives\n",
    "fp = elementwise_grad(f, 1)\n",
    "fpp = elementwise_grad(fp, 1)\n",
    "\n",
    "eta = np.linspace(-5,5,50)[:, None]\n",
    "\n",
    "\n",
    "\n",
    "# This is the function we seek to minimize\n",
    "def objective(params, step):\n",
    "    zeq = -0.5*fpp(params, eta) + (0.5* eta**2 - 1.5065724128094344 )* f(params, eta)\n",
    "    bc0 = f(params, -5.0)  # equal to zero \n",
    "    bc1 = f(params, -1.0) + 0.644288  #0.644288 # value at 1 \n",
    "    bc2 = f(params,  1.0) - 0.644288\n",
    "    bc3 =  f(params, 5.0)\n",
    "    y1 = f(params, eta)**2\n",
    "    prob = np.sum((y1[1:] + y1[0:-1]) / 2 * (eta[1:] - eta[0:-1]))\n",
    "    prob = 1.0\n",
    "    return np.mean(zeq**2) + bc0**2 +   bc2**2 + bc1**2 + bc3**2 + (1. -prob)**2\n",
    "\n",
    "def callback(params, step, g):\n",
    "    if step % 1000 == 0:\n",
    "        print(\"Iteration {0:3d} objective {1}\".format(step,\n",
    "                                                      objective(params, step)))\n",
    "       \n",
    "params = adam(grad(objective), params,\n",
    "              step_size=0.001, num_iters=15000, callback=callback)\n",
    "\n",
    "\n",
    "psi = (1/np.pi)**0.25 * np.sqrt(2) * eta * np.exp(-eta**2 / 2)\n",
    "\n",
    "print('f(0) = {}'.format(f(params, -5.)))\n",
    "print('fp(0) = {}'.format(fp(params, -5.)))\n",
    "print('f(5) = {}'.format(f(params, 5.0)))\n",
    "                                                      \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(eta, f(params, eta), 'b', label = \"prediction\")\n",
    "plt.plot(eta, psi, 'r', label = \"analytical\")\n",
    "##plt.ylabel('psi & truth')\n",
    "##plt.xlim([0, 14])\n",
    "##plt.ylim([-1., 1.])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
